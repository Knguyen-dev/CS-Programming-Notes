{"cells":[{"cell_type":"markdown","id":"prostate-arizona","metadata":{"id":"bA5ajAmk7XH6"},"source":["# Cleaning Data in Python\n","This was done as a part of datacamp's curriculum. It's pretty useful to be reminded of the basic techniques."]},{"cell_type":"code","execution_count":2,"id":"2e25fdd8-4d84-45bc-80f0-949917e00a17","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":4240,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1725127196693,"lastExecutedByKernel":"b79560cc-8272-4c70-9a3e-7fed4950f7ac","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the course packages\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport fuzzywuzzy\nimport recordlinkage \n\n# Import the course datasets\nride_sharing = pd.read_csv('datasets/ride_sharing_new.csv', index_col = 'Unnamed: 0')\nairlines = pd.read_csv('datasets/airlines_final.csv',  index_col = 'Unnamed: 0')\nbanking = pd.read_csv('datasets/banking_dirty.csv', index_col = 'Unnamed: 0')\nrestaurants = pd.read_csv('datasets/restaurants_L2.csv', index_col = 'Unnamed: 0')\nrestaurants_new = pd.read_csv('datasets/restaurants_L2_dirty.csv', index_col = 'Unnamed: 0')"},"outputs":[],"source":["# Import the course packages\n","import pandas as pd\n","import numpy as np\n","import datetime as dt\n","import matplotlib.pyplot as plt\n","import missingno as msno\n","import fuzzywuzzy\n","import recordlinkage \n","\n","# Import the course datasets\n","ride_sharing = pd.read_csv('datasets/ride_sharing_new.csv', index_col = 'Unnamed: 0')\n","airlines = pd.read_csv('datasets/airlines_final.csv',  index_col = 'Unnamed: 0')\n","banking = pd.read_csv('datasets/banking_dirty.csv', index_col = 'Unnamed: 0')\n","restaurants = pd.read_csv('datasets/restaurants_L2.csv', index_col = 'Unnamed: 0')\n","restaurants_new = pd.read_csv('datasets/restaurants_L2_dirty.csv', index_col = 'Unnamed: 0')"]},{"cell_type":"markdown","id":"ea3556f0","metadata":{},"source":["# Common Data problems\n","\n","## Converting from integer to category\n","It's the idea that we're changing a column from representing quantitative data to categorical data. This makes sense when a column represents a fixed set of values like enums, or a discrete set e.g. role column is 'User', 'Admin', 'Moderatotr', etc. Doing this can help reduce memory usage, increase performance, and makes our data a lot cleaner.\n"]},{"cell_type":"code","execution_count":null,"id":"079c7628-464c-471b-9867-b684acc232b9","metadata":{},"outputs":[],"source":["# Creates a new column 'user_type_cat' by converting an existing column to a categorical type, \n","ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype(\"category\")\n","\n","# This just makes sure that our changes were actually applied to the data frame. Else our script throws an error\n","assert ride_sharing['user_type_cat'].dtype.name == 'category'"]},{"cell_type":"markdown","id":"e78b527a-a1e0-4106-ad88-9ded50a57428","metadata":{},"source":["## Ensuring that we are working with numbers instead of strings"]},{"cell_type":"code","execution_count":null,"id":"2490e71c-fe5d-40d8-9847-724f32feeebf","metadata":{},"outputs":[],"source":["'''\n","1. Strip duration column of 'minutes' string; essential to integer conversion as we can't really convert something like '  15  ' to an integer \n","until we get rid of the spaces.\n","2. Convert duration_trim to integer\n","'''\n","ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip(\"minutes\")\n","ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype(int)\n","assert ride_sharing['duration_time'].dtype == \"int\"\n","\n","print(ride_sharing[['duration', 'duration_trim', 'duration_time']])\n","print(ride_sharing['duration_time'].mean())\n"]},{"cell_type":"markdown","id":"855ee204-d9ba-4fb4-8dc4-7301a0e2a1ad","metadata":{},"source":["## Range of your data \n","Sometimes data is supposed to be in a set range of values. Like the registration date for a user can't be in the future, or how the ratings for a movie review can only be from 1 to 5.\n","\n","### How to deal with out of range data:\n","\n","1. How to deal with out of range data: Drop the data, but this is only recommended when there's only a few rows that have out of range data. \n","\n"]},{"cell_type":"code","execution_count":null,"id":"a6a23c69-97cb-4330-846e-ccbaf7eafce5","metadata":{},"outputs":[],"source":["'''\n","- Ex.1: Bicycle tire sizes can be 26, 27, or 29. They're stored as 'categorical' values instead of numerical for some reason. However the company is now setting the new maximum tire size to be 27.\n","\n","- Steps:\n","1. Convert tire_sizes to integer\n","2. Set all tire size values above 27, to 27.  \n","3. Remember the business rule is that tire sizes are categories rather than actual numbers to do calculations with. So convert the tire_sizes series \n","back to categorical data. \n","'''\n","ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype'(int') \n","ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n","ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n","print(ride_sharing['tire_sizes'].describe())"]},{"cell_type":"code","execution_count":null,"id":"cbd908b5-cbd3-48ff-b6d4-9c31b9486833","metadata":{},"outputs":[],"source":["'''\n","- Ex. 2: The data pipeline feeding into our 'ride_sharing' data frame has been updated to register hwen a ride is registered. This is done in the 'ride_date' column, which is of type 'object', which is treated as strings in Pandas. A bugged happened with registration dates, so just for this one time, ensure that any values for 'ride_date' has a maximum datetime of 'today'.\n","\n","- Steps:\n","1. It's normal for dates to be strings (object type) when first imported into a Pandas DataFrame. So we'll convert all date values in the series to a datetime object, so that we can do date related operations.\n","2. Save today's date as a datetime object into a variable\n","3. Set all future dates to today's date\n","4. Print hte maximum value from the ride_dt column.\n","'''\n","ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n","today = dt.date.today()\n","ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n","print(ride_sharing['ride_dt'].max())"]},{"cell_type":"markdown","id":"da09ba68-240d-4a37-b8e1-07cd9e0b7bd8","metadata":{},"source":["## Unique constraints and duplicate values \n","\n","### What are duplicate values?\n","Of course a user can have the same first or last name as another user, so when we talk about duplicate values we're saying that 'most of the columns have duplicate values'. Like the first name, last name, the address, and even the weight of the users are the same. The only thing different is height, but still that's pretty suspicious. Probably a data-entry error, human error, bug or design error from data pipeline/source, or even a result of badly merging stuff.\n","\n","\n","### How to find duplicate values properly\n","SQL columns by default allow duplicate values, so how do we find duplicates and not get mislead by normal data? Us ethe `.duplicated()` method, but pass in arguments to its parameters. This function returns a series of indices that are associated with duplicate rows.\n","- subset: List of column names to check for duplication.\n","- keep: Whether to keep first ('first'), last ('last'), or all (False) duplicate values.\n","```\n","column_names = ['first_name','last_name','address']\n","duplicates = df.duplicated(subset=column_names, keep=False)\n","# Sorting the data so that it's easier to see\n","df[duplicates].sort_values(by='first_name')\n","```\n","So if two or more rows have the same first_name, last_name, and address columns, then they're included in the resulting 'duplicates' data frame. We do 'keep=False' so that we get all rows.\n","- complete duplicate: Rows in a dataframe that are identical across all columns.\n","- incomplete duplicate: Rows in a dataframe that are identical based on a subset of columns, rather than all columns.\n","- \n","### Treating duplicate values\n","1. Use the `df.drop_duplicates()` method. This returns a dataframe with the duplicate rows removed. A by default, rows are considered duplicate when all of their columns match, but you can specify a subset so that if all of the columns in that subset match, then the rows are removed. You can specify the 'keep' argument to keep the first row whilst removing all duplicates, or pass in 'last' to keep the last row. Just don't do 'False' since that just keeps all of the duplicates, and you aren't really achieving anything with that.\n","2. You can also attempt to combine duplicate rows. If two rows are duplicates, but maybe the 'weight' values are different. You can compute the mean 'weight' value, and put that all in one row. Whether you use mean, min, max, or some other statistical way depends on your data situation.\n","```\n","column_names = ['first_name','last_name','address']\n","summaries = {'height':'max','weight','mean'}\n","height_weight = df.groupby(by=column_names).agg(summaries).reset_index() \n","\n","duplicates = height_weight.duplicated(subset=column_name, keep=False)\n","\n","# Shouldn't have any duplicate rows show up.\n","height_weight[duplicates].sort_values(by='first_name')\n","```"]},{"cell_type":"code","execution_count":null,"id":"baf2dd7d-d0dd-42f2-8cfa-89b74648b43c","metadata":{},"outputs":[],"source":["'''\n","- Ex. 1: Finding duplicates\n","1. Find duplicated rows of ride_id. Set keep to 'False' so you actually see all of the duplicate values.\n","2. Sort the duplicate rows by ride_id.\n","3. Print the ride_id, duration, and user_birth_year from the duplicate rides rows \n","'''\n","duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n","duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n","print(duplicated_rides[['ride_id','duration','user_birth_year']])"]},{"cell_type":"code","execution_count":null,"id":"4dbfbbe1-042b-4965-94dc-9884270310fe","metadata":{},"outputs":[],"source":["'''\n","- Ex. 2: Treating duplicates\n","\n","1. Drop complete duplicates from ride_sharing. If the ride_id, user_birth_year, and durations are equal, then drop it. T\n","2. Create statistics dictionary for aggregation function since we plan to get summary statistics. So we're getting the minimum birth year, and getting the mean for 'duration'.\n","3. Group the rows by 'ride_id', so we'll create groups, and each group will have rows that have matching ride_id values. Then for the rows in each group, \n","we'll calculate summary statistics for the user birth year and duration. We do 'reset_index()' to convert the 'groupby' object back itno a dataframe.\n","'''\n","ride_dup = ride_sharing.drop_duplicates(subset=['ride_id','user_birth_year','duration'])\n","statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n","ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n","\n","'''\n","4. Attempt to find any remaining duplicates based on 'ride_id'. Pass in 'keep=False' to get all duplicates, and then all rows with duplicates \n","are marked with true. Remember this is a series of the row indexes where 'True' means duplicate, and false means it's not a duplicate.\n","5. Filter the the ride_unique to get al lrows where ride_id is duplicated. Storing them in duplicated rides.\n","'''\n","# Find duplicated values again\n","duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n","duplicated_rides = ride_unique[duplicates == True]\n","\n","# Assert duplicates are processed\n","assert duplicated_rides.shape[0] == 0"]},{"cell_type":"markdown","id":"8f66d5cc-35ce-483f-b9bc-aea2c82e0960","metadata":{},"source":["# Handling text and categorical data\n","\n","## Membership constraints\n","Categorical data has 'discrete' values or values in a finite set of values. For example, 'Marriage Status' can only be 'unmarried' or 'married'. Something like 'Loan Status' could only have values 'default', 'payed', or 'no_loan'. \n","\n","There could be issues with categorical data, where the categories are invalid, missing, etc. Let's see how we can treat these \n","\n","### How to deal with category \n","1. Dropping data\n","2. Remapping categories\n","3. Inferring the category value based on other values in the row\n","\n","### Pandas Joins\n","Allows you to combine data from two DataFrames based on a common column or index. This operation is similar to SQL joins and is useful for merging datasets that share some relationship. There are main four types of joins:\n","1. Inner Join: `result = pd.merge(df1,df2, on=\"some_column\", how=\"inner\"`. So it'll return the rows from both df1 and df2, where the 'some_column' values match.\n","2. Left Join: `result = pd.merge(df1, df2, on='key_column', how='left')`. So it'll return All rows from the left DataFrame (df1), and matching rows from df2. If there were no matching rows from df2, for a particular row in df1, then the rows from df2 will be NaN.\n","3. Right Join: Same idea as left join, but with the right side.\n","4. Outer Join: Returns all rows from both DataFrames, and matches where possible. If there's no match, then NaN is used for missing values.\n","\n","Given two data frames, you can do join operations\n","- Anti-joins: Return columns that are in dataframe A and not in dataframe B.\n","- Inner joins: Return columns that are in both A and B.\n","\n","### Errors we can have\n","\n","#### Value inconsistency\n","- Inconsistent fields: 'married', 'Maried', 'UNMARRIED', 'not married '. You can fix some of this by upper or lowercasing everything.\n","- Trailing or leading whitespace. Fix this by getting rid of the whitespace\n","In general you're using `.str.strip()`, `.str.lower()`, and `.str.upper()`\n","\n","#### Creating or remapping categories\n","- You can create categories\n","```\n","# Create category names and ranges  \n","group_names = [\"0-200K\", \"200K-500K\", \"500K+\"]\n","ranges = [0, 200000, 500000, np.inf]\n","\n","demographics['income_group'] = pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n","demographics[['income_group', 'household_income']]\n","```\n","And the output would look like this:\n","```\n","     income_group    household_income\n","0    0-200K          189243\n","1    500K+           778533\n","```\n","- You can also map categories to fewer ones, in order to reduce the amount of categories in a given column. So here, we replace any entries containing 'Microsoft', 'MacOS', and 'Linux' to 'DesktopOS'. You get the idea. And then we do .unique() to print out all of the unique values of the 'operating_system' series!\n","```\n","mapping = {\n","    \"Microsoft\": \"DesktopOS\",\n","    \"MacOS\": \"DesktopOS\",\n","    \"Linux\": \"DesktopOS\",\n","    \"IOS\": \"MobileOS\",\n","    \"Android\": \"MobileOS\"\n","}\n","devices['operating_system'] = devices['operating_system'].replace(mapping)\n","devices['operating_system'].unique()\n","```\n"]},{"cell_type":"code","execution_count":3,"id":"f1c46727-069d-4bef-be24-b955ce7c5209","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":54,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1725127215616,"lastExecutedByKernel":"b79560cc-8272-4c70-9a3e-7fed4950f7ac","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"'''\n- Ex. 1: The main thing is the idea that we're printing out the unique values in each of the columns. This allows us to \n\n'''\nprint(\"Cleanliness: \", airlines['cleanliness'].unique(), \"\\n\")\nprint(\"Safety: \", airlines['safety'].unique(), \"\\n\")\nprint(\"Satisfaction: \", airlines['satisfaction'].unique(), \"\\n\")\n","outputsMetadata":{"0":{"height":164,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Cleanliness:  ['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty'] \n","\n","Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n","\n","Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n"," 'Very unsatisfied'] \n","\n"]}],"source":["'''\n","- Ex. 1: The main thing is the idea that we're printing out the unique values in each of the columns. This allows us to see the possible values for these \n","columns. Assume we have a 'categories' data frame that contains all the correct possible values of the survey columns.\n","\n","'''\n","print(\"Cleanliness: \", airlines['cleanliness'].unique(), \"\\n\")\n","print(\"Safety: \", airlines['safety'].unique(), \"\\n\")\n","print(\"Satisfaction: \", airlines['satisfaction'].unique(), \"\\n\")\n","\n","# Find all categories values in the first set that aren't in the categories['cleanliness'] series; this finds all invalid categories\n","cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n","\n","# Get all rows with invalid categories; this just returns a series of index and true or false\n","cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n","\n","# Plug that into the dataframe to get all the rows that have an invalid category\n","print(airlines[cat_clean_rows])"]},{"cell_type":"code","execution_count":null,"id":"ad82f0fa-a522-48aa-9978-5e360f2f7248","metadata":{},"outputs":[],"source":["'''\n","- Ex. 2: Here the main idea is that we make all of the values in 'dest_region' lowercased, and also we remapped a value 'eur'. So 'eur' represents 'europe', but we already have a 'europe' series value, so remap all rows with 'eur' to 'europe'.\n","'''\n","# Print unique values of both columns\n","print(airlines['dest_region'].unique())\n","print(airlines['dest_size'].unique())\n","\n","# Lower dest_region column and then replace \"eur\" with \"europe\"\n","airlines['dest_region'] = airlines['dest_region'].str.lower() \n","airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n","\n","# Remove white spaces from `dest_size`\n","airlines['dest_size'] = airlines['dest_size'].str.strip()\n","\n","# Verify changes have been effected\n","print(airlines['dest_region'])\n","print(airlines['dest_size'])"]},{"cell_type":"code","execution_count":null,"id":"35c2102f-ee32-416e-b63c-e7bdd0537523","metadata":{},"outputs":[],"source":["'''\n","- Ex. 3: \n","\n","- Rules: \n","1. wait_type: 'short' for 0-60 min, 'medium' for 60-180 and long for 180+. We have an existing 'wait_mins' column you can use.\n","2. day_week: 'weekday' if day is in the weekday, 'weekend' if day is in the weekend. We have an existing 'day' column that you can derive from.\n","\n","demographics['income_group'] = pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n","'''\n","\n","# Create ranges for categories\n","label_ranges = [0, 60, 180, np.inf]\n","label_names = ['short', 'medium', 'long']\n","\n","# Create wait_type column based on the 'wait_min'\n","airlines['wait_type'] = pd.cut(airlines['wait_min'], bins=label_ranges, labels=label_names)\n","\n","\n","# Create mappings and replace\n","mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n","            'Thursday': 'weekday', 'Friday': 'weekday', \n","            'Saturday': 'weekend', 'Sunday': 'weekend'}\n","airlines['day_week'] = airlines['day'].replace(mappings)"]},{"cell_type":"markdown","id":"5b551227-20ad-4a42-b6db-828c464dfd1a","metadata":{},"source":["## Cleaning text data\n","\n","Text data consists of things such as names, phone numbers, emails, passwords, and any other data that's going to be represented as strings.\n","\n","### Common text data problems\n","- Inconsistency: Is the phone number represented as `+96171679912` or `0096171679912`. We need to have a uniform way of processing it.\n","- Constraint violatiosn: Passwords need to be at least 8 characters, and meet other conditions.\n","- Typos: A phone number with a typo like `+961.471.679912` needs to be fixed.\n","\n","#### Phone number example\n","Let's say that we want all of our phone numbers in the form `00xxxxxxxxxxx`. Let's say some of our data is in form `+x-xxx-xxx-xxxx`. Let's convert things. Let's also handle the case where incomplete phone numbers like `xx-xxxx` or something similar is converted to \n","```\n","phones['Phone number'] = phones['Phone number'].str.replace(\"+\", \"00\")\n","phones['Phone number'] = phones['Phone number'].str.replace(\"-\", \"\")\n","\n","# Returns a series of the lengths of each phone number\n","digitLength = phones['Phone number'].str.len()\n","\n","# digitLength < 10 creates a boolean series.\n","# Then phones.loc selects all rows in the 'Phone number' column where the condition is true, derived from the series value.\n","phones.loc[digitLength < 10, \"Phone number\"] = np.nan \n","```\n"]},{"cell_type":"code","execution_count":null,"id":"90842750-217f-480c-a25f-194ef4feca9f","metadata":{},"outputs":[],"source":["'''\n","- Ex. 1: Do some string manipulate to remove honorifics from the 'full_name' column.\n","1. Replace \"Dr.\" with empty string \"\"\n","2. Replace \"Mr.\" with empty string \"\"\n","3. Replace \"Miss\" with empty string \"\"\n","4. Replace \"Ms.\" with empty string \"\"\n","'''\n","\n","airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n","airlines['full_name'] = airlines[\"full_name\"].str.replace(\"Mr.\", \"\")\n","airlines[\"full_name\"] = airlines['full_name'].str.replace(\"Miss\", \"\")\n","airlines['full_name'] = airlines[\"full_name\"].str.replace(\"Ms.\", \"\")\n","\n","# Assert that full_name has no honorifics\n","assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"]},{"cell_type":"code","execution_count":null,"id":"16dfd98f-e51e-41b8-9011-44d44e1ce0ab","metadata":{},"outputs":[],"source":["'''\n","- Ex. 2: Do some string manipulate to remove honorifics from the 'full_name' column.\n","1. Store length of each row in survey_response column\n","2. Find rows in airlines where resp_length > 40\n","3. Assert minimum survey_response length > 40\n","4. Print values of the the new 'survey_response' \n","'''\n","resp_length = airlines['survey_response'].str.len()\n","airlines_survey = airlines[resp_length > 40]\n","assert airlines_survey['survey_response'].str.len().min() > 40\n","print(airlines_survey['survey_response'])"]},{"cell_type":"markdown","id":"673046d0-7758-47b2-84ac-cf76888622ef","metadata":{},"source":["# Advanced data problems\n","\n","Addressing issues such as ensuring all weights are in kilograms instead of pounds. Then we'll go into more practice and detail to ensure that missing values don't negatively impact our analysis. \n","\n","\n","## Unit Uniformity\n","We need to unit uniformity. Data such as temperature needs to all be in Celsius or Fahrenheit, pick one! If we have weights in kilograms, we should know how to be able to programmatically convert those values into pounds or other forms as well. The same goes for dates and also currency.\n","\n","\n","#### Examples\n","Here's an example based on temperatures. We can assume that the `temperatures` DataFrame has values in fahrenheit by default\n","```\n","# We can assume that values above 40 represent fahrenheit\n","temp_fahrenheit = temperatures.loc[temperatures['Temperature'] > 40, 'Temperature']\n","\n","# Apply a function on the series?\n","temp_celsius = (temp_fahrenheit - 32) * (5/9)\n","\n","# Assert that the conversion is correct\n","temperatures.loc[temperatures['Temperature'] > 40, 'Temperature'] = temp_celsius\n","assert temperature ['Temperature'].max() < 40\n","```\n","Here's an example based on dates. Assume that we have 3 formats for dates `27/27/19`, `03-29-19`, or `March 3rd, 2019`. \n","```\n","\n","\n","birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'], infer_datetime_format=True, )\n","\n","```\n","### Treating ambiguous date data\n","Is `2019-03-08` in August or March. Here are a couple of ways you can handle this:\n","1. Convert to Na or missing and treat accordingly.\n","2. Infer the format by understanding the data source.\n","3. Infer the format by understanding previous and subsequent data in DataFrame.\n","4. "]},{"cell_type":"code","execution_count":null,"id":"016c4f4d-140e-45c7-b3bb-167e4b03b8a4","metadata":{},"outputs":[],"source":["'''\n","+ Ex. 1: We're working with banking data. So let's do a simple conversion example\n","\n","- Data columns:\n","1. acct_amount: Amount of money stored in the accounts\n","2. acct_cur: Currency type\n","3. inv_amount: Amount invested\n","4. account_opened: Account opening date. When the account was made.\n","5. last_transaction: Date of the most recent transaction\n","''''\n","\n","# Find values of acct_cur that are equal to 'euro'; our filter\n","acct_eu = banking['acct_cur'] == \"euro\"\n","\n","# conversion rate from euro to usd \n","conversion_rate = 1.1\n","\n","# For every row that has a 'euro' currency, update its 'acct_amount' to be in USD\n","banking.loc[acct_eu, \"acct_amount\"] = banking.loc[acct_eu, \"acct_amount\"] * conversion_rate \n","\n","# For every row with the euro currency, convert the currency type to dollar\n","banking.loc[acct_eu, \"acct_cur\"] = banking.loc[acct_eu, \"acct_cur\"] = \"dollar\"\n","\n","# Assert that only dollar currency remains\n","assert banking['acct_cur'].unique() == ['dollar']"]},{"cell_type":"code","execution_count":null,"id":"bb1afedf-aee4-4d4b-ba03-bab375646956","metadata":{},"outputs":[],"source":["'''\n","+ Ex. 2: Handling the converting of date formats. So let's uniformalize the date formatting.\n","So when you first try pd.to_datetime() on the 'account_opened' series, you get this error\n","'ValueError: month must be in 1..12'. Alongside seeing the output, it just means that our dates aren't in a uniform format, which causes the error when we try to change the strings into date objects.\n","\n","\n","Before output: \n","    0          2018-03-05\n","    1            21-01-18\n","    2    January 26, 2018\n","    3            21-14-17\n","    4            05-06-17\n","    \n","- How does 'infer_datetime_format' work?\n","Instructs Pandas to automatically guess the date format of each string in the series, as this is built in to handle a variety of formats such as:\n","1. YYYY-MM-DD (e.g., \"2018-03-05\")\n","2. DD-MM-YY or MM-DD-YY (e.g., \"21-01-18\" or \"05-06-17\")\n","3. Month DD, YYYY (e.g., \"January 26, 2018\")\n","\n","Also Pandas is generally good at parsing a widely variety of date formats, including those with different delimiters such as '/', '-', or spaces. Dates are converted into 'yyyy-mm-dd' format.\n","\n","After output:\n","0   2018-03-05\n","1   2018-01-21\n","2   2018-01-26\n","3          NaT, '14' isn't a valid month\n","4   2017-05-06\n","Name: account_opened, dtype: datetime64[ns]\n","'''\n","\n","# Print the header of account_opened\n","print(banking['account_opened'].head())\n","\n","# Convert account_opened to datetime\n","banking['account_opened'] = pd.to_datetime(\n","    banking['account_opened'],\n","    infer_datetime_format = True,\n","    errors = 'coerce'\n",")\n","\n","# Get year of account opened\n","banking['acct_year'] = banking['account_opened'].dt.strftime(\"%Y\")\n","\n","# Print acct_year\n","print(banking['acct_year'])"]},{"cell_type":"markdown","id":"10607c64-ef1f-4b01-9cf3-93994b67db21","metadata":{},"source":["## Cross field validation\n","The use of fields to check other fields, or you can think of it as using multiple fields to ensure your data is valid. For example using the 'first_class', 'business_class', and 'economy_class' and summing those up to make sure that they match the 'total_passengers' field!\n","```\n","sum_classes = flights['economy_class','business_class','first_class'].sum(axis=1)\n","\n","passengers_equals = sum_classes == flights['total_passengers']\n","\n","inconsistent_passengers_rows = flights[~passengers_equals]\n","consistent_passengers_rows = flights[passengers_equals]\n","```\n","Another example could be comparing a user's 'age' vs their 'Birthday' field.\n","```\n","import pandas as pd\n","import datetime as dt\n","\n","# Convert all string values to date time objects\n","users['Birthday'] = pd.to_datetime(users['Birthday'])\n","today = dt.date.today();\n","\n","# Manually calculate the age for every value in the series. Then create a filter\n","calculated_age = today.year - users['Birthday'].dt.year\n","age_equals = calculated_age == users['Age']\n","\n","# Find and filter out rows with inconsistent and consistent ages\n","inconsistent_age_rows = users[~age_equals]\n","consistent_age_rows = users[age_equals]\n","```\n","\n","### How to treat inconsistencies\n","\n","1. Dropping data\n","2. Set to missing and maybe impute it (derive it from other fields).\n","3. Apply rules or what you know about your dataset that's appropriate for your specific situation.\n"]},{"cell_type":"code","execution_count":null,"id":"5d8a926d-366e-495f-bdfc-cdd04752dfd5","metadata":{},"outputs":[],"source":["'''\n","+ Ex. 1: Maintaining data integrity. Ensuring the funds all sum up to the investment amount associated with the account. Ensuring also the ages were good.\n","'''\n","# Store fund columns to sum against\n","fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n","sum_funds = banking[fund_columns].sum(axis=1)\n","\n","# Find rows where fund_columns row sum == inv_amount\n","inv_equ = sum_funds == banking['inv_amount']\n","\n","# Store consistent and inconsistent data\n","consistent_inv = banking[~inv_equ]\n","inconsistent_inv = banking[inv_equ]\n","\n","# Convert all string values to date time objects\n","banking['age'] = pd.to_datetime(banking['birth_date'])\n","today = dt.date.today();\n","ages_manual = today.year - banking['birth_date'].dt.year\n","age_equals = ages_manual == banking['Age']\n","\n","inconsistent_ages = banking[~age_equals]\n","consistent_ages = banking[age_equals]"]},{"cell_type":"markdown","id":"3e2e5d7f-8f8c-42e5-8d4f-48adfbc73255","metadata":{},"source":["## Missing Data\n","When no data value is stored for a value in a row. Missing data is typically represented by 'NA', 'nan', 0, or '.' This commonly happens due to a technical or human error.\n","\n","### Correlations between missing data\n","1. Missing Completely at Random (MCAR)\n","- Definition: Data is missing completely at random when the likelihood of a data point being missing is unrelated to any other data, including both observed and unobserved data.\n","- Example: Imagine you're collecting survey data, and some respondents accidentally skipped a question due to a random software glitch. The missing data doesn't depend on any specific trait of the respondent or other answers they've given; it's purely random.\n","- Implication: Since there's no pattern to the missing data, it doesn't bias the analysis. However, you may lose some statistical power because of the reduced sample size.\n","2. Missing at Random (MAR)\n","- Definition: Data is missing at random when the missingness is related to other observed data but not to the value of the data that is missing itself.\n","- Example: Suppose you're analyzing a dataset of students' grades, and you notice that older students are more likely to have missing grades because they tend to skip more classes. The missing grades are related to the age of the students (which is observed), but not to the actual grades themselves.\n","- Implication: The missing data can introduce bias if not properly accounted for. However, because the missingness is related to other observed variables, techniques like imputation (filling in missing data based on observed data) can be used effectively.\n","3. Missing Not at Random (MNAR)\n","- Definition: Data is missing not at random when the missingness is related to the value of the data itself that is missing. In other words, there's a systematic relationship between the propensity of missingness and the unobserved data.\n","- Example: Consider a survey question about income where high-income individuals are more likely to leave the question unanswered because they feel it's too personal. The likelihood of missing data is directly related to the income level, which is the value of the missing data itself."]},{"cell_type":"code","execution_count":null,"id":"8178b082-f699-42ff-b774-e7b729736ae9","metadata":{},"outputs":[],"source":["'''\n","+ Ex. 1: \n","\n","'''\n","# Print number of missing values in banking\n","print(banking.isna().sum())\n","\n","# Visualize missingness matrix\n","msno.matrix(banking)\n","plt.show()\n","\n","# Isolate missing and non missing values of inv_amount\n","missing_investors = banking[banking['inv_amount'].isna()]\n","investors = banking[~banking['inv_amount'].isna()]\n","\n","# Sort banking by age and visualize\n","banking_sorted = banking.sort_values(by = 'age')\n","msno.matrix(banking_sorted)\n","plt.show()"]}],"metadata":{"editor":"DataLab","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}
