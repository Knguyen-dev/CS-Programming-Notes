# Math and Science

Notes about math and science that I wanted to take notes on. I may forget things. 

- How do orthogonal vectors relate to the independence of features in machine learning?
- Why are orthogonal matrices used in dimensionality reduction techniques like PCA?
- How does the Gram-Schmidt process ensure numerical stability in computations?


# Credits 
1. [Essence of linear algebra - 3b1b](https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&si=rMF6XORkg3GRoOvx)
2. [Calculus Formula Sheet/CheatSheet](https://drive.google.com/file/d/1k8iKVh9qwM_Bwv0WsBaUR33vSo6mNHGk/view?usp=sharing)
2. [Essence of calculus](https://youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&si=vqu1Pc0f3vpM1K78)
3. [Multivariable Calculus - Khan Academy](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function)
4. [AP/College Statistics - Khan Academy](https://www.khanacademy.org/math/statistics-probability)

5. Definitely khan academy linear algebra for the orthogonal stuff. And I may have to just teach myself the singular vector decomposition stuff.